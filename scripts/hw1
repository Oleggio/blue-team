1. Create a folder for input data in your HDP sandbox, create python or bash script to create folder in HDFS and remove same folder.

hadoop fs -mkdir /user/admin/dataset

2. Load dataset into your HDFS in sandbox using distcp, write a python or bash script that loads data from local system folder and writes it to HDFS
   If the source datasource were another HDFS or distributed object storage, then we would use DISTCP. But in our case we load dataset from local filesystem.
  Copy to HDP Sandbox:
	scp -P 2222 -pr ~/projects/datasets/ root@sandbox-hdp.hortonworks.com:/root/hdp/datasets
  Upload to HDFS:
  	hadoop fs -put /root/hdp/datasets/ /user/admin/

  Alternitevly we can mount HDFS and copy the files directly to the sandbox.

3. Get and validate(run) supporting web app. It uses Play Web Framework and written in Scala.
   Port is optional(9000 by default).
	git clone https://github.com/Grogs/scala-course.git
	sbt "~run 9001"

4. 