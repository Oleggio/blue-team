1. Create a folder for input data in your HDP sandbox, create python or bash script to create folder in HDFS and remove same folder.

	hadoop fs -mkdir /user/admin/datasets
	hadoop fs -rm -r /user/admin/datasets

2. Load dataset into your HDFS in sandbox using distcp, write a python or bash script that loads data from local system folder and writes it to HDFS
   If the source datasource were another HDFS or distributed object storage, then we would use DISTCP. But in our case we load dataset from local filesystem.
  Copy to HDP Sandbox:
	scp -P 2222 -pr ~/projects/datasets/ root@sandbox-hdp.hortonworks.com:/root/hdp/datasets
  Upload to HDFS:
  	hadoop fs -put /root/hdp/datasets/ /user/admin/

  Alternitevly we can mount HDFS and copy the files directly to the sandbox.

3. Get and validate(run) supporting web app. It uses Play Web Framework and written in Scala.
   Port is optional(9000 by default).
	git clone https://github.com/Grogs/scala-course.git
	sbt "~run 9001"

4. Analyze supporting web application. Create a docker image for this application, create docker file that will create image on web app build (can use maven plugin for that)
   Added SBT support to build an image as well(at the team`s GitHub version).
	sbt docker:publishLocal - creates the app image in the local directory
   To run app:
	docker run --rm -p9000:9000 play-scala-server:1.0-SNAPSHOT
